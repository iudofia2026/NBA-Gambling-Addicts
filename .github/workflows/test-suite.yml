name: NBA Betting Model Test Suite

on:
  push:
    branches: [ main, prod-2, prod, dev, develop ]
  pull_request:
    branches: [ main, prod-2, prod ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
        cache-dependency-path: |
          requirements.txt
          tests/requirements.txt

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-mock pytest-xvfb
        pip install pandas numpy scikit-learn

    - name: Run unit tests
      run: |
        pytest tests/unit/ -v \
          --cov=src \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing \
          --tb=short \
          -m "not slow and not integration"

    - name: Upload coverage to Codecov
      if: matrix.python-version == '3.10'
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unit-tests
        name: codecov-unit-tests
        fail_ci_if_error: false

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    if: success()

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-mock

    - name: Run integration tests
      run: |
        pytest tests/integration/ -v \
          --cov=src \
          --cov-append \
          --cov-report=xml \
          --tb=short \
          -m "integration"

    - name: Upload integration coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: integration-tests
        name: codecov-integration-tests
        fail_ci_if_error: false

  data-validation:
    name: Data Validation Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    if: success()

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest

    - name: Run data validation tests
      run: |
        python -m pytest tests/unit/test_data_integrations.py::TestDataSourcesIntegration -v
        python -m pytest tests/integration/test_data_flow_validation.py -v
      env:
        PYTHONPATH: ${{ github.workspace }}/src

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: github.event_name == 'schedule' || github.ref == 'refs/heads/main' || github.ref == 'refs/heads/prod-2'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-benchmark memory-profiler

    - name: Run performance tests
      run: |
        pytest tests/unit/test_data_integrations.py::TestDataPerformance -v \
          --benchmark-only \
          --benchmark-json=benchmark.json

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: benchmark.json

  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black isort mypy

    - name: Lint with flake8
      run: |
        # Stop the build if there are Python syntax errors or undefined names
        flake8 src --count --select=E9,F63,F7,F82 --show-source --statistics
        # Exit-zero treats all errors as warnings
        flake8 src --count --exit-zero --max-complexity=10 --max-line-length=100 --statistics

    - name: Check code formatting with black
      run: |
        black --check --diff src tests

    - name: Check import sorting with isort
      run: |
        isort --check-only --diff src tests

    - name: Type check with mypy
      continue-on-error: true
      run: |
        mypy src --ignore-missing-imports --no-strict-optional

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install bandit
      run: |
        pip install bandit[toml]

    - name: Run security scan
      run: |
        bandit -r src -f json -o bandit-report.json || true

    - name: Upload security scan results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-scan-results
        path: bandit-report.json

  test-report:
    name: Generate Test Report
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, data-validation]
    if: always()

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        pip install pytest pytest-html

    - name: Generate test report
      run: |
        pytest tests/ -v \
          --html=test-report.html \
          --self-contained-html \
          --tb=short || true

    - name: Upload test report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-report
        path: test-report.html

  notify:
    name: Notify Results
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, data-validation, code-quality]
    if: always()

    steps:
    - name: Notify success
      if: ${{ needs.unit-tests.result == 'success' && needs.integration-tests.result == 'success' }}
      run: |
        echo "✅ All tests passed successfully!"
        echo "Unit Tests: ${{ needs.unit-tests.result }}"
        echo "Integration Tests: ${{ needs.integration-tests.result }}"
        echo "Data Validation: ${{ needs.data-validation.result }}"
        echo "Code Quality: ${{ needs.code-quality.result }}"

    - name: Notify failure
      if: ${{ needs.unit-tests.result == 'failure' || needs.integration-tests.result == 'failure' }}
      run: |
        echo "❌ Some tests failed!"
        echo "Unit Tests: ${{ needs.unit-tests.result }}"
        echo "Integration Tests: ${{ needs.integration-tests.result }}"
        echo "Data Validation: ${{ needs.data-validation.result }}"
        echo "Code Quality: ${{ needs.code-quality.result }}"
        exit 1